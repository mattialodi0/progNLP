{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0467e434",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3500b41",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6ef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\matti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd1ca6",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b79268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_json(name):\n",
    "    from collections import Counter\n",
    "    with open(name, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index')\n",
    "        df = df[['id_EXIST', 'lang', 'tweet', 'labels_task2']]\n",
    "        df = df.rename(columns={'labels_task2': 'label'})\n",
    "        df = df[df['lang'] == 'en']\n",
    "\n",
    "        def most_frequent_or_drop(arr):\n",
    "            if not isinstance(arr, list):\n",
    "                return arr\n",
    "            c = Counter(arr)\n",
    "            most_common = c.most_common()\n",
    "            if len(most_common) == 0:\n",
    "                return None\n",
    "            max_count = most_common[0][1]\n",
    "            candidates = [val for val,\n",
    "                          count in most_common if count == max_count]\n",
    "            if len(candidates) > 1:\n",
    "                return None\n",
    "            return candidates[0]\n",
    "\n",
    "        df['label'] = df['label'].apply(most_frequent_or_drop)\n",
    "\n",
    "        def map_label(label):\n",
    "            mapping = {\n",
    "                '-': 0,\n",
    "                'DIRECT': 1,\n",
    "                'JUDGEMENTAL': 2,\n",
    "                'REPORTED': 3\n",
    "            }\n",
    "            return mapping.get(label, None)\n",
    "        \n",
    "        df = df.dropna(subset=['label'])\n",
    "        df['label'] = df['label'].apply(map_label)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = dataset_from_json('data/training.json')\n",
    "val_df = dataset_from_json('data/validation.json')\n",
    "test_df = dataset_from_json('data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0102d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>200001</td>\n",
       "      <td>en</td>\n",
       "      <td>FFS! How about laying the blame on the bastard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>200004</td>\n",
       "      <td>en</td>\n",
       "      <td>@GMB this is unacceptable. Use her title as yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200005</th>\n",
       "      <td>200005</td>\n",
       "      <td>en</td>\n",
       "      <td>‘Making yourself a harder target’ basically bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_EXIST lang                                              tweet  label\n",
       "200001   200001   en  FFS! How about laying the blame on the bastard...      0\n",
       "200002   200002   en  Writing a uni essay in my local pub with a cof...      3\n",
       "200003   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...      3\n",
       "200004   200004   en  @GMB this is unacceptable. Use her title as yo...      0\n",
       "200005   200005   en  ‘Making yourself a harder target’ basically bo...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e7cae2",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991e072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(df):\n",
    "    def clean_text(text):\n",
    "        # text = text.lower() # not required\n",
    "        text = emoji.replace_emoji(text, replace='') # remove emojis\n",
    "\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # remove URLs\n",
    "        text = re.sub(r'\\@\\w+|\\#','', text) # remove mentions and hashtags\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text) # remove special characters\n",
    "        text = re.sub(r'\\s+', ' ', text) # remove extra spaces\n",
    "        text = re.sub(r'\\\"\\'\\`\\’\\‘\\“\\”', ' ', text) # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    # remove invalid chars\n",
    "    df['tweet'] = df['tweet'].apply(clean_text)\n",
    "\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        def get_wordnet_pos(tag):\n",
    "            if tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return wordnet.NOUN  # Default to noun if unknown\n",
    "\n",
    "        words = text.split()\n",
    "        pos_tags = pos_tag(words)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    df['tweet'] = df['tweet'].apply(lemmatize_text)\n",
    "    return df\n",
    "\n",
    "train_df = text_preprocessing(train_df)\n",
    "val_df = text_preprocessing(val_df)\n",
    "test_df = text_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af61fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_EXIST</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200001</th>\n",
       "      <td>200001</td>\n",
       "      <td>en</td>\n",
       "      <td>FFS How about lay the blame on the bastard who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200002</th>\n",
       "      <td>200002</td>\n",
       "      <td>en</td>\n",
       "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200003</th>\n",
       "      <td>200003</td>\n",
       "      <td>en</td>\n",
       "      <td>it be not I dont appreciate that on two ride b...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200004</th>\n",
       "      <td>200004</td>\n",
       "      <td>en</td>\n",
       "      <td>this be unacceptable Use her title a you do fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200005</th>\n",
       "      <td>200005</td>\n",
       "      <td>en</td>\n",
       "      <td>Making yourself a hard target basically boil d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_EXIST lang                                              tweet  label\n",
       "200001   200001   en  FFS How about lay the blame on the bastard who...      0\n",
       "200002   200002   en  Writing a uni essay in my local pub with a cof...      3\n",
       "200003   200003   en  it be not I dont appreciate that on two ride b...      3\n",
       "200004   200004   en  this be unacceptable Use her title a you do fo...      0\n",
       "200005   200005   en  Making yourself a hard target basically boil d...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e19465",
   "metadata": {},
   "source": [
    "## Task3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e69184",
   "metadata": {},
   "source": [
    "### Create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5830d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2873/2873 [00:00<00:00, 125634.52it/s]\n"
     ]
    }
   ],
   "source": [
    "idx_to_word = OrderedDict()\n",
    "word_to_idx = OrderedDict()\n",
    "\n",
    "curr_idx = 0\n",
    "for sentence in tqdm(train_df.tweet.values):\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in word_to_idx:\n",
    "            word_to_idx[token] = curr_idx\n",
    "            idx_to_word[curr_idx] = token\n",
    "            curr_idx += 1\n",
    "word_listing = list(idx_to_word.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea97a23",
   "metadata": {},
   "source": [
    "evaluate vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6d4841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12347/12347 [00:00<00:00, 1447496.20it/s]\n"
     ]
    }
   ],
   "source": [
    "assert len(idx_to_word) == len(word_to_idx)\n",
    "assert len(idx_to_word) == len(word_listing)\n",
    "for i in tqdm(range(0, len(idx_to_word))):\n",
    "    assert idx_to_word[i] in word_to_idx\n",
    "    assert word_to_idx[idx_to_word[i]] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355840a",
   "metadata": {},
   "source": [
    "save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83cce132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vocabulary to ./data/vocab.json\n",
      "Saving completed!\n"
     ]
    }
   ],
   "source": [
    "vocab_path = './data/vocab.json'\n",
    "\n",
    "print(f\"Saving vocabulary to {vocab_path}\")\n",
    "with open(vocab_path, mode='w') as f:\n",
    "    json.dump(word_to_idx, f, indent=4)\n",
    "print(\"Saving completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
