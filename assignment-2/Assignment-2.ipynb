{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sexism Detection, Multi-class Classification, LLMs, Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFN9Q2rGLJnc"
   },
   "source": [
    "\n",
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN4nmQacznGV"
   },
   "source": [
    "# Relevant Material\n",
    "\n",
    "- Tutorial 3\n",
    "- Huggingface documentation\n",
    "- Huggingface hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck47wFhrLJnc"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "You are tasked to address the [EDOS Task B](https://github.com/rewire-online/edos) on sexism detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mtjp0cGLJnc"
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "Given an input text sentence, the task is to label the sentence as non-sexist or one of these four sexist categories: (1) threats, (2)  derogation, (3) animosity, (4) prejudiced discussion.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "**Text**: *``Schedule a date with her, then don't show up. Then text her \"GOTCHA B___H\".''*\n",
    "\n",
    "**Label**: Threats\n",
    "\n",
    "**Text**: *``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.''*\n",
    "\n",
    "**Label**: Not sexist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dqwRZx-QNGX"
   },
   "source": [
    "## Approach\n",
    "\n",
    "We will tackle the five-class classification task with LLMs.\n",
    "\n",
    "In particular, we'll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PS3igwXpQcAY"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "We are going to download LLMs from [Huggingface](https://huggingface.co/).\n",
    "\n",
    "Many of these open-source LLMs require you to accept their \"Community License Agreement\" to download them.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- If not already, create an account of Huggingface (~2 mins)\n",
    "- Check a LLM model card page (e.g., [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)) and accept its \"Community License Agreement\".\n",
    "- Go to your account -> Settings -> Access Tokens -> Create new token -> \"Repositories permissions\" -> add the LLM model card you want to use.\n",
    "- Save the token (we'll need it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqEsPH_JSxw6"
   },
   "source": [
    "### Huggingface Login\n",
    "\n",
    "Once we have created an account and an access token, we need to login to Huggingface via code.\n",
    "\n",
    "- Type your token and press Enter\n",
    "- You can say No to Github linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uWEUjs0THxP",
    "outputId": "7626b973-c7b0-475a-fee9-030a72b4042d"
   },
   "outputs": [],
   "source": [
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLxSrY-4e_0J"
   },
   "source": [
    "After login, you can download all models associated with your access token in addition to those that are not protected by an access token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEYMBnAQLJnc"
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "Since we are only interested in prompting, we do not require a train dataset.\n",
    "\n",
    "We have preparared a small test set version of EDOS in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material).\n",
    "\n",
    "Check the ``Assignment 2/data`` folder.\n",
    "It contains:\n",
    "\n",
    "- ``a2_test.csv`` → a small test set of 300 samples.\n",
    "- ``demonstrations.csv`` -> a batch of 1000 samples for few-shot prompting.\n",
    "\n",
    "Both datasets contain a balanced number of sexist and not sexist samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhWnrpY90xKs"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qP3rQML42EBz",
    "outputId": "96033e37-04f7-46d5-e665-fbc38a700bdd"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install evaluate\n",
    "!pip install bitsandbytes\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0Ug46NOTTND"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from google.colab import drive\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pv0o1VR9mZf5",
    "outputId": "1f4c2301-2255-4022-f518-6ea7b50c0df1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5XyOcFGLJnd"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "We require you to:\n",
    "\n",
    "* **Download** the ``A2/data`` folder.\n",
    "* **Encode** ``a2_test.csv`` into a ``pandas.DataFrame`` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ge_7bydWysS2",
    "outputId": "64076eae-c257-4b9b-d7e8-6dd575b744d7"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/nlp-unibo/nlp-course-material.git\n",
    "!ls nlp-course-material/2025-2026/Assignment\\ 2/data\n",
    "\n",
    "data_path = \"nlp-course-material/2025-2026/Assignment 2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hgLHeitB0nLY",
    "outputId": "a66aa873-b012-4cca-9d70-cc97cdbef1ab"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{data_path}/a2_test.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UL1d3_A2uI2"
   },
   "source": [
    "The data is perfectly balanced as we can see from the followign code. In fact, each class is equally represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liwFOnD91BIl",
    "outputId": "0655e195-35e6-4b21-b5be-0781a36efde1"
   },
   "outputs": [],
   "source": [
    "print(df['label_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJp08l4yLJnd"
   },
   "source": [
    "# [Task 1 - 0.5 points] Model setup\n",
    "\n",
    "Once the test data has been loaded, we have to setup the model pipeline for inference.\n",
    "\n",
    "In particular, we have to:\n",
    "- Load the model weights from Huggingface\n",
    "- Quantize the model to fit into a single-GPU limited hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptffotFIjq89"
   },
   "source": [
    "### Which LLMs?\n",
    "\n",
    "The pool of LLMs is ever increasing and it's impossible to keep track of all new entries.\n",
    "\n",
    "We focus on popular open-source models.\n",
    "\n",
    "- [Mistral v2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "- [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "- [Llama v3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "- [Phi3-mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
    "- [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)\n",
    "- [Qwen3](https://huggingface.co/Qwen/Qwen3-1.7B)\n",
    "\n",
    "Other open-source models are more than welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH1YShLfLJnd"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 1 points, we require you to:\n",
    "\n",
    "* Pick 2 model cards from the provided list.\n",
    "* For each model:\n",
    "  - Setup a quantization configuration for the model.\n",
    "  - Load the model via HuggingFace APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8ORld7klCfG"
   },
   "source": [
    "### Note\n",
    "\n",
    "There's a popular library integrated with Huggingface's ``transformers`` to perform quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVL89TcOSA85"
   },
   "source": [
    "### Our implementation\n",
    "\n",
    "We chose to use the Mistral v3 model and the Llama v3.1 using the quantization configuration seen during the tutorial on prompting.\n",
    "\n",
    "In the following code snippets we load, quantize and print the structure of the two models. We do also load the tokenizers corresponding to the used models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWjvWRcpLAhc"
   },
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorWithPadding,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957,
     "referenced_widgets": [
      "6eac8f5cf77b439b9aab92a699d2ab62",
      "dec2f2c919ed4e03a07603e50051f9d1",
      "8693d2b16b984214bfe441d5baf6a89f",
      "507746725e38449f9544f93007a077d8",
      "615828fc3e5e4da8a4c6f9348035d72c",
      "b3a53e7a98e04da6bf16f01f4ca1b8a3",
      "5201e0f84e4148f5a04328fb29a493ef",
      "d6527cbb20674088b0b3e4363d71557b",
      "af847f2096db4e998381e9e88d825fca",
      "64d75f49cdfb4e84b464973c33ab613f",
      "c3775a4f9ddf44b2a84333dd8462df39",
      "ccd3be88fa484bac98e9a359e48e66d8",
      "f9768de34f8c4444b8966ac744bf7ffa",
      "325990384b294b1a8fe43f7e65479dbe",
      "38897b0f987c4bd88e842a33ff74d2c7",
      "b98acc442b69488180bd1311b1aa8af7",
      "9444a73bbc7c4904a418e1343c24e427",
      "3f96c0fa3edf450ebd985cb8f4d6f753",
      "71f6d688883d4fe990678423a80856b1",
      "82bbe2a2a8fd443e9110a9261ea9d839",
      "f90b7d46ed36424bbf407e52cd8b42de",
      "256d12d82db14fbab7c7e6bbeaa083b5",
      "4f55cb9483fe42a28e63b211d9ed28e5",
      "2a896e9e227f424298e8935f0fedf4f5",
      "0708e534467848148f0d31aaea7a463d",
      "6d805b7110a64332a7429df5fa6b72d5",
      "1f829fea418f408992d4538087a29e79",
      "a2751eed14ea4efdbfed676b19e87e34",
      "427130f843704f15968fae78ebcc409d",
      "eec6db2120d74114ac9199d1bc3de0f3",
      "f000cd764ba74300866117102d5c2297",
      "2ba962e99a8a4b42bae4c18d9b79de02",
      "51fad31eb1c445f6b81098eb1965b907",
      "4a8fda1d6a8646ea84c4d84d5c377fb6",
      "435c6389469c4c7ea7500c92f0964b64",
      "7dd73c24571942d4a8f4178d643b68c4",
      "a76865a8f4fa44b4872cf282c3d9a76b",
      "365acb5a2a8f4fe48fcf746c7b8a3b46",
      "fdef5cfae7ad43b2bb061e51e5210750",
      "67904dd89dcd4529b3a77a623b7f134a",
      "b7243ef2cc9c475ebe10ca198161279c",
      "c195447a591844b1a1c8ac6030c01cbd",
      "7c25c854ac154a6397bffe8b7e1c0e71",
      "2389bbe50b534becb4c680d38d9fe519",
      "20b5d6064e4746e2acfc5a1613c5a3ee",
      "14e1d147326f469e9891c46dbc84201e",
      "21f7cdf49648444891ed076d46ddfb70",
      "4fbd56a583134b67b555ac72252745e8",
      "9ccacd5fda744eda865fb2182644295f",
      "47956ea4152349bd9f7d78fa6ab85017",
      "9cd02ad0672d4e83b90440470d674c48",
      "d31de68945cc43d7a76f7850684bc5fd",
      "60e33b2f337240328eba1734f7a061df",
      "ac513e8ff607474080533f6944681f68",
      "1b64ffad0f604d58ad9ce6b522ec38ed",
      "a8f97e1050084d7e96071cd47f9726c0",
      "6c1d5c067f7541f7b4ba3d3518718ad3",
      "2b3e435cb62b4cc1b37e65ae25fa7dce",
      "f16961a781cf43149d3bd19009221155",
      "aa3d4160976f4dd0ac3fe73fe46fe8a7",
      "8b70c2c3313d4ad2b172a8b396319a69",
      "4ced878e12914e1c8d7bb8df5e3cc35a",
      "d3253298a608437a8033f1f3f0e213dd",
      "1d623e403a2a45c3a7774e6ae665b08b",
      "7eebc8d99292403b85edfacd573795db",
      "df30274de7dd4d87bf9f1da967273822",
      "a20326298c72485a9686f144d1df4ef1",
      "e7562bf2d66d406fb3917bc001f1e090",
      "b9aa40973ebf48989cd171f69d93a30b",
      "c136be0a36104a2fb798e309e4c3b4c7",
      "6f862a763df149578329df8deab001bc",
      "2cdfd219983e442781dd6fa51c5b7a80",
      "285555dabf774afaae876e79a9641d67",
      "ef452f8d4ecf4834885edd84cfa7f58c",
      "fa23a51c92fe429f9891e22cab0031f6",
      "7d151691026e4aa4b3193a55cd8063fe",
      "dee99431721845f1a0c9a79798b16af8",
      "1cc83089bdc2451c9a29f19585274195",
      "618c0dd86cfa415b97b0f132f08290ea",
      "c793732ff6cb43e8a45819f674e93bff",
      "f04fd2e2d87f4df9af3f4b3f2c08a1e7",
      "c591c3faa031405bad9824d4e9ce1804",
      "a24c6262abd043d3ba7dca0d583a965e",
      "e8893f02fc044df2b52f18f3ca22690c",
      "68969ca230dd4e518ddb3d7f21ef31dd",
      "46bd940c3efe413baa47be68e468c47e",
      "d5029db2cda44aef87c04f8877a90dca",
      "8325b08c837a4bc796b24a50136aaecd",
      "32b7ac80521140f1bf1d3be263b8bb86",
      "0751352be32d442d95cf65bb8f8fc07a",
      "475cb27350db42a3a2d62b9662e4da47",
      "ecc6f3f6803a464380f92579d8068baa",
      "fdfa3c34825445f2b7767bf02360de4a",
      "9f2c56a1865e4292b928cb4aa2172f3e",
      "db31baa6c27446cb8e8a1a224d781e6c",
      "597cb069bde64e01bef77c45bf5aac2b",
      "0ad52f2f07464d90aa143e2130ef88ed",
      "b8c6b5e9589c4f1a82a390c603baca39",
      "9c800f0dd212483199e1c99cc3b6efb4",
      "33bcf957a00b4074a3b899930ffd3107",
      "83fafe3a22764f158f89fc97dc6f15e3",
      "133d7115f0094cf9951d5c790465a35e",
      "c53049edaa2949f59ae36cfd3d2e4711",
      "9b14d96f1f9745b588799c96849250c0",
      "395968f1bcca42cea24834f4ab526bf6",
      "49cb23b8ed544f5a9a1e154ad012ae17",
      "bf44e5607d04441eb26b8b2577779705",
      "6e265f16d31248fca3db40ab78ffaa4e",
      "0b0e36e58ff7475e960ddda3c33fbb6f",
      "a1143b8b972b4d888f62010bdc3436d5",
      "cf7d6208abd44dc398508a1df73f7e59",
      "af351bf56fbd45b998fc3e88c90df616",
      "96bd49e14885413881ab7903ea46eac4",
      "cad32e60174d4848a22fcc87199af385",
      "99cf6b84aded4983bdaf7fc1496b805a",
      "fa8626d2f639486396caeca5d9dee31c",
      "ff22dd15dc4046d299e475d0c4f669e7",
      "c185a967be204bcabbcff63c9c2bac2f",
      "d775068a1e5c4a0fb59ac2b2a19c8b1c",
      "9bd5f55871a54eafa5b7b1b535be2198",
      "664ebf731438474ebb6877f9e40ed46d",
      "6173aa93a4d145e8a8114734dc2d2c09",
      "97f6afb409ad46afb527ca02d67a6b0b",
      "1671c948813c4b69ad4e786294ed59fb",
      "865c1ea289434f35b12a5bc549dce987",
      "3b962faa11384d529fd0ca6e62bb54b1",
      "16d71456cbc14c2fa3e7165463f0ca60",
      "6dd43e49fbcb49c98d19cb96471003f6",
      "b472d38b498f44dc9c7512a2a822973d",
      "642e5fb9c6834c3d94f6745663c77637",
      "018f21bee151437a939b495b2a5f36cb",
      "e28cf18efba1414885947458e5f5e492"
     ]
    },
    "id": "EUnOFUkp1hij",
    "outputId": "d57c08aa-bf5f-476a-f5e3-43bde5a69dec"
   },
   "outputs": [],
   "source": [
    "mistral_card = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_card)\n",
    "mistral_tokenizer.padding_token = mistral_tokenizer.eos_token\n",
    "\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_card,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Configure generation for Mistral\n",
    "mistral_gen = mistral_model.generation_config\n",
    "mistral_gen.max_new_tokens = 40\n",
    "mistral_gen.eos_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_gen.pad_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_gen.temperature = None\n",
    "mistral_gen.num_return_sequences = 1\n",
    "\n",
    "print(mistral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyOfOtHdHa-V"
   },
   "outputs": [],
   "source": [
    "# qwen_card = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_card)\n",
    "\n",
    "# qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     qwen_card,\n",
    "#     return_dict=True,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map='auto'\n",
    "# )\n",
    "\n",
    "# # Configure generation for Qwen\n",
    "# qwen_gen = qwen_model.generation_config\n",
    "# qwen_gen.max_new_tokens = 40\n",
    "# qwen_gen.eos_token_id = qwen_tokenizer.eos_token_id\n",
    "# qwen_gen.pad_token_id = qwen_tokenizer.eos_token_id\n",
    "# qwen_gen.temperature = None\n",
    "# qwen_gen.num_return_sequences = 1\n",
    "\n",
    "# print(qwen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852,
     "referenced_widgets": [
      "1043e6c61cae40859bd7e4e4b071a89a",
      "da1839a0adb64c1e9f431d47ca0a88aa",
      "6d31adcb4feb41d88cf3d25bfbf6dc82",
      "ada533b9eb174163af62e2261362dda7",
      "0736bfaebf114819bb52b9a161daac7f",
      "c69808bd8fd24e269bb07f0e3ed13f91",
      "8ff473b1de9f4ee4b79c9963f11daada",
      "b5fd1ec27f2441caaa63e64d2e435ee9",
      "e8791d99ff18465b8dc0b7209c76ba3d",
      "babc07097ba547b7899dd5c0a6d86511",
      "07249df5a0d1426c9c57f903bb97dc4b",
      "3f16e8ec43c541188ca266581faed15b",
      "487e54dbb37a4fd7967b70ac3e7c664d",
      "8ac87abc139d439394edae9e538e8da4",
      "5460383609894dd787f187b92c19af14",
      "d222389a19f840bc82ca8212eb694d7a",
      "c6ac2e6ae73d48dab310da57e4892c34",
      "64e0de8fadd64ccf8a2794fff4c198df",
      "f24b9b6b415a46a99491a7dd473c9e3f",
      "d89aebd0d28342d88b50bfe45b3c373c",
      "c51d0b214bac4b0cbc5faeca1e60c67a",
      "e437ed7f1c50494ba0a7f3d5083d5c2e",
      "16196f0cadc44fe7aad6595d26f11bf6",
      "f43f8f2633864bc98a3beb7e3d43f19a",
      "48a36ebf0db0454d8d9446578af99164",
      "6e9adb575ca445efa6190a0ea4bb5aed",
      "8825f0a0d7c847ad97e930f75e4f0f4d",
      "0724a125fad54f7ca3a6a5c9354c1821",
      "effdf385e1bf484d94fc45f927016c49",
      "2755bdfcefcb4c61908c0eaf2323dd5e",
      "048b3de680e24d7aa58ff3dc82f42cb5",
      "4ad7a834a2b54c04814b72ddcd7812dd",
      "ca3f17842cae407eb1afae279dac175d",
      "143ead4c11c5450da719c5bfba340f75",
      "f2a4547ac12d480fa9b7414198a59f08",
      "c96c3eed7bc644a0a676110c10200ea1",
      "96abe98a1d69427a89f2d39bb8cdc886",
      "b403e43202bf43a0b978941062f06b7c",
      "ed5a0089107d40e9910e14183a5889d4",
      "568740d82857459089c5c2afc664b587",
      "55af3b2b0dd84e239069233ea634af58",
      "4ae6619f801e46d9840ccc4d1e3952e3",
      "329c9a9d290d43b1bd640cbbe770ff0e",
      "2540e9f52fe14f0cad8b5124b27b6ade",
      "4139db0884ec47acae67f6bef5e82e75",
      "339138acb66b40a0be025c394438984f",
      "c1ee052477ed4c0d8e4a8a23781f0117",
      "0123da57e25b4b5085c4e28a2bced1b6",
      "657703c5150c49ebb2fe37fa5639e34a",
      "8812c824e4904d498ce16d74c23b3997",
      "ef9937bad64f48f992e7509fd6958c1a",
      "a2cbf5577cf84cb280420bdb206ebb2d",
      "bec5c10efe4246be932d9debe09b3973",
      "bb190f835c6d49308297289125af8a12",
      "a4c2437ee7bd418eb210f4602da5c931",
      "83274f4e323d4d2a8981ec0d5519843f",
      "a9df6d65b6524bd0b96b82901699fd93",
      "e522f7fbded24f40b0ae548822200cd3",
      "85278074cd6c48079aed60b43b230ee7",
      "d1cc07e1c1934ef58bf1e8ff0fd73251",
      "7f88e8a0337f41b08ebc0cbfcc395413",
      "8a70a43fabce4f608ac5aa0241fea51e",
      "da73032cf1a7438f94552f7a8555d0a6",
      "bd43ef2520cf4d30941fcfb59bd0121b",
      "37eff19fc5414801b07f14daa6b7e541",
      "33cc826f26c241cf9ee237fc5021afd1",
      "69ca27053d974bf58d8efbbc8454eaa6",
      "00c4ccd44f1645f8add841509ef5f25d",
      "bdd0dccaa0f0472fb65de238866d5bb8",
      "4579f7a37bb84bd299c2b37970cac829",
      "52ffae4367ea43d6a3e40565ef745796",
      "97cfe3f0fcf64a1b94145203f7368f83",
      "df3219c50b0b4165a35a74f523aab208",
      "c65f0517a6964be98073850451571571",
      "f7b31445b4f547deb63f9e11ca53ee29",
      "d0d5948fefbc4f6ebc1f1716a4f4992e",
      "7f0654f8226649db8ec3a6108cd078c9",
      "d2a3541fefe648b3b48d204f8e2e5f05",
      "c544a0c1da6542368f58f1a9a118db4d",
      "4f5e4013a1e94b9cbd32a06a932f7513",
      "bed201a78d9a49498069d1de0bff0d45",
      "360183ae20ca43cba976d40d7d0e21aa",
      "3cc39cc515414e759a6b4a7d77ebdbff",
      "7b137aecb55b40658156246c63365bec",
      "677c40435bfe48cfa09b5a5290c68398",
      "aebe9bb05df04c2690814e890b5b9318",
      "6a4af4f885f148e89e2a4b33be30eaaf",
      "a99e41c57a934fa9823fd478ea4cf729",
      "b65d398aeb5f4f8ea36062faaa62fdb2",
      "7279f424a68c459d9917de0647c4ea6b",
      "7e36688d573c4c559211c223299410d8",
      "1f977192461049b49d4177cead0f7b96",
      "56980acad5bd494193bf3715fd4325ef",
      "a48bbd915a2f4efe95e5abbf238b4427",
      "00992037605d4ad593423a0ee2a03b9c",
      "39ce2a158a6845169db82192cef228e3",
      "d754cc6b90da49f8883f1d43b2f00284",
      "10909ca23d924022bcde81a735f875d0",
      "1e451638467b47f3836e226ab7e0b2bd",
      "1840b0f97ca842ada1c27c1869b331cf",
      "8969bb18382e475b9e6c2986a45ee0bb",
      "ff92e9360bab4de88baf35d992e73509",
      "ff94adc5564a454db4353632377cf862",
      "b92c344bf19540139053ede4a00899f9",
      "40fac93516274605b2862c4d48c2f59b",
      "8a2df35eeaa54ccdb28b5b269fbbb638",
      "ec8b8cf1a63149f5b214691a1e09f07a",
      "86c29717f97646e0992b83cc20588598",
      "223f15866a254a5fa23c076621deb7bb",
      "ffb92caf47bd4b9b8f30393acfadc4aa",
      "15c2e101225246f1bec1db83527347dd",
      "2e89ebdcb3a0401ca6c0556ed569f86a",
      "58635fd45a7341519004656dc579b509",
      "c1ad354bb4734d5db49696381fd53786",
      "d8fcd95d80ec4e0186d95e456ad6e28c",
      "087dfa020ec240aab0916d416cded254",
      "793ff57e55ee40b680641c286122aa3f",
      "9f390ca841c846be9b86ea970f681112",
      "57c99b03c6fc42359d3d7d3cf35d32b6",
      "8a179157c8464bff87a875e0e5395a87",
      "0215c242fcef4bba981792d002f560c1",
      "d841f0b8e364442cb94c1a31c3878f26",
      "861f6d2939444a60855b9e0b34e5082f",
      "323fd9d379c84979a083f4a97a79b4bf",
      "d631ed668e8a430bb3d195374465bbfb",
      "bf674097603d4030afb31a5f156745ae",
      "6b4044e3b28744878f3fac8b869fb39f",
      "772a1767ed6c4d31af145816ec3e6c0b",
      "a82da5a1dacd4de39abc62c13bc34631",
      "f98e798fdfde4ed49a02cae8b047b58b",
      "6fc2f809d3a048ffa5e7ddb8bc18c32a",
      "5b769820f3ab41deae05c746775fed43"
     ]
    },
    "id": "EnOqx7RkEWte",
    "outputId": "c2fff91f-a011-4874-a667-a76a2a98506c"
   },
   "outputs": [],
   "source": [
    "llama_card = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_card)\n",
    "llama_tokenizer.padding_side = \"left\"\n",
    "llama_tokenizer.padding_token = llama_tokenizer.eos_token\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_card,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "llama_gen = llama_model.generation_config\n",
    "llama_gen.max_new_tokens = 40\n",
    "llama_gen.eos_token_id = llama_tokenizer.eos_token_id\n",
    "llama_gen.pad_token_id = llama_tokenizer.eos_token_id\n",
    "llama_gen.temperature = None\n",
    "llama_gen.num_return_sequences = 1\n",
    "\n",
    "print(llama_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzNNzb1VLJnd"
   },
   "source": [
    "# [Task 2 - 1.0 points] Prompt setup\n",
    "\n",
    "Prompting requires an input pre-processing phase where we convert each input example into a specific instruction prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GaBtKXomY_m"
   },
   "source": [
    "### Prompt Template\n",
    "\n",
    "Use the following prompt template to process input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e8P-Kk8me6q"
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sexism detection.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Your task is to classify input text as not-sexist\n",
    "         or sexist. If sexist, classify input text according to one\n",
    "         of the following four categories: threats, derogation,\n",
    "         animosity, prejudiced discussion.\n",
    "\n",
    "         Below you find sexist categories definitions:\n",
    "         Threats: the text expresses intent or desire to harm a woman.\n",
    "         Derogation: the text describes a woman in a derogative manner.\n",
    "         Animosity: the text contains slurs or insults towards a woman.\n",
    "         Prejudiced discussion: the text expresses supports for\n",
    "         mistreatment of women as individuals.\n",
    "\n",
    "         Respond only by writing one of the following categories:\n",
    "         not-sexist, threats, derogation, animosity, prejudiced.\n",
    "\n",
    "        TEXT: {text}\n",
    "\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHeoEN7MLJnd"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 2 points, we require you to:\n",
    "\n",
    "* Write a ``prepare_prompts`` function as the one reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je45EJwMRySo"
   },
   "source": [
    "### Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUjRVMtMm9CE"
   },
   "outputs": [],
   "source": [
    "def prepare_prompts(texts, prompt_template):\n",
    "  \"\"\"\n",
    "    This function format input text samples into instructions prompts.\n",
    "\n",
    "    Inputs:\n",
    "      texts: input texts as a list of strings to classify via prompting\n",
    "      prompt_template: the prompt template provided in this assignment\n",
    "\n",
    "    Outputs:\n",
    "      input texts to classify in the form of instruction prompts\n",
    "  \"\"\"\n",
    "\n",
    "  formatted_prompts = []\n",
    "  for text in texts:\n",
    "    formatted_prompts.append(prompt_template.format(text=text))\n",
    "\n",
    "  return formatted_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeLp4ap2-LEx"
   },
   "source": [
    "Below we visualize the effects of the `prepare_prompt` function using both the tokenizers. We can see the differences between the two tokenizers in how they organize the prompt adding special tokens and additional information as the cutting knowledge date of the llama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-rq46ya-Caw4",
    "outputId": "6b70c8cb-a2a2-435a-828a-162c7424383d"
   },
   "outputs": [],
   "source": [
    "mistral_prompt = mistral_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "llama_prompt = llama_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "sample_texts = df['text'].values[:1]\n",
    "print(sample_texts)\n",
    "prepared = prepare_prompts(sample_texts, llama_prompt)\n",
    "for i in range(len(sample_texts)):\n",
    "  print(prepared[i])\n",
    "\n",
    "prepared = prepare_prompts(sample_texts, mistral_prompt)\n",
    "for i in range(len(sample_texts)):\n",
    "  print(prepared[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOCRgGf7mifk"
   },
   "source": [
    "### Notes\n",
    "\n",
    "1. You are free to modify the prompt format (**not its content**) as you like depending on your code implementation.\n",
    "\n",
    "2. Note that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgBhkBwuLJnd"
   },
   "source": [
    "# [Task 3 - 1.0 points] Inference\n",
    "\n",
    "We are now ready to define the inference loop where we prompt the model with each pre-processed sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WsrQSvcLJnd"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 3 points, we require you to:\n",
    "\n",
    "* Write a ``generate_responses`` function as the one reported below.\n",
    "* Write a ``process_response`` function as the one reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYgGACUiRtSl"
   },
   "source": [
    "### Our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXRB-wwx_cc8"
   },
   "source": [
    "First, we define a function that based on the used model provides a response extractor function that cleans the answer of the LLM outputing a single label category, as required by the classification task we are tackling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfcwCHaXQj0z"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_response_warped(model_name):\n",
    "\n",
    "  '''\n",
    "  The function returns a function based on the model we are using. The returned function processes the raw response of the model extracting only the label category.\n",
    "\n",
    "  Inputs:\n",
    "    model_name: string with the name of the model we are using\n",
    "        response: input of the internal function and is the raw response of the model\n",
    "\n",
    "  Outputs:\n",
    "    extract_response: the function that cleans the answer\n",
    "        The internal function returns the label category of the answer.\n",
    "  '''\n",
    "\n",
    "  model_name = model_name.lower()\n",
    "\n",
    "  if model_name=='mistral':\n",
    "    def extract_response(response):\n",
    "      match = [m for m in re.finditer('ANSWER:', response)][-1]\n",
    "      parsed = response[match.end():].strip()\n",
    "      label_only = parsed.split('(')[0].strip()\n",
    "\n",
    "      # The model outputs prejudiced discussion as indicated in the prompt definitions, we change to prejudiced to be coherent with the labels\n",
    "      if label_only == 'prejudiced discussion':\n",
    "        label_only = 'prejudiced'\n",
    "\n",
    "      # The model outputs in some cases more categories. Select the first one in this case\n",
    "      if ',' in label_only:\n",
    "        label_only = label_only.split(',')[0]\n",
    "\n",
    "      return label_only\n",
    "\n",
    "  elif model_name=='llama':\n",
    "    def extract_response(response):\n",
    "      LABEL_PATTERN = re.compile(\n",
    "        r\"(not-sexist|threats|derogation|animosity|prejudiced)\",\n",
    "        flags=re.IGNORECASE\n",
    "      )\n",
    "      matches = LABEL_PATTERN.findall(response)\n",
    "      return matches[-1]\n",
    "\n",
    "  return extract_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMi6bkn2AxaB"
   },
   "source": [
    "Below we implement the two recquired functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIZ2iq0hUQsO"
   },
   "outputs": [],
   "source": [
    "def generate_responses_batched(model, tokenizer, prompt_examples, model_name):\n",
    "  \"\"\"\n",
    "    This function implements the inference loop for a LLM model.\n",
    "    Given a set of examples, the model is tasked to generate\n",
    "    a response.\n",
    "\n",
    "    Inputs:\n",
    "      model: LLM model instance for prompting\n",
    "      tokenizer: the tokenizer corresponding to the model\n",
    "      prompt_examples: pre-processed text samples in the form of a DataLoader\n",
    "      model_name: string with the name of the model we are using,\n",
    "                  necessary to get the cleaning function\n",
    "\n",
    "    Outputs:\n",
    "      generated responses\n",
    "  \"\"\"\n",
    "\n",
    "  responses = []\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    for batch_text, batch_label in tqdm(prompt_examples, desc=\"Inference\"):\n",
    "\n",
    "      input_ids = batch_text['input_ids'].to(model.device)\n",
    "      attention_mask = batch_text['attention_mask'].to(model.device)\n",
    "\n",
    "      response = model.generate(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          generation_config=model.generation_config,\n",
    "          do_sample=False\n",
    "      )\n",
    "      raw_response = tokenizer.batch_decode(response,\n",
    "                                            skip_special_tokens=True)\n",
    "      extractor = extract_response_warped(model_name)\n",
    "      raw_response = [extractor(resp) for resp in raw_response]\n",
    "      responses.extend(raw_response)\n",
    "  return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiCoMrutyXTU"
   },
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "  \"\"\"\n",
    "    This function takes a textual response generated by the LLM\n",
    "    and processes it to map the response to a binary label.\n",
    "\n",
    "    Inputs:\n",
    "      response: generated response from LLM\n",
    "\n",
    "    Outputs:\n",
    "      parsed classification response.\n",
    "      Use the following mapping:\n",
    "      {\n",
    "        'not-sexist': 0,\n",
    "        'threats': 1,\n",
    "        'derogation': 2,\n",
    "        'animosity': 3,\n",
    "        'prejudiced': 4\n",
    "      }\n",
    "  \"\"\"\n",
    "\n",
    "  text2class = {\n",
    "      'not-sexist': 0,\n",
    "      'threats': 1,\n",
    "      'derogation': 2,\n",
    "      'animosity': 3,\n",
    "      'prejudiced': 4\n",
    "  }\n",
    "  return text2class[response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr7s3-jZR2GC"
   },
   "source": [
    "We define below the collate function that will be passed to the `DataLoader` instance. Moreover we add two columns to the dataset where the prepared prompts for each model are stored and create the `DataLoader` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPbpDdditO46"
   },
   "outputs": [],
   "source": [
    "def collate_fn(tokenizer):\n",
    "\n",
    "  def process(batch):\n",
    "    texts = tokenizer.batch_encode_plus(\n",
    "        [ex['text'] for ex in batch],\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = torch.tensor([ex['label'] for ex in batch])\n",
    "    return {'input_ids': texts['input_ids'],\n",
    "            'attention_mask': texts['attention_mask']}, labels\n",
    "\n",
    "  return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "m9EknQ6_oZry",
    "outputId": "8174c736-8e3f-446c-a3f0-6b944310c3ab"
   },
   "outputs": [],
   "source": [
    "# Create new columns in the dataframe for the framed texts\n",
    "df['framed_text_mistral'] = prepare_prompts(df['text'].values, mistral_prompt)\n",
    "df['framed_text_llama'] = prepare_prompts(df['text'].values, llama_prompt)\n",
    "df['label'] = df.apply(lambda x: process_response(x['label_category']), axis=1)\n",
    "\n",
    "mistral_data = Dataset.from_pandas(df[['framed_text_mistral', 'label']].rename(\n",
    "    columns={'framed_text_mistral': 'text'}\n",
    "))\n",
    "llama_data = Dataset.from_pandas(df[['framed_text_llama', 'label']].rename(\n",
    "    columns={'framed_text_llama': 'text'}\n",
    "))\n",
    "\n",
    "print(mistral_data)\n",
    "print(llama_data)\n",
    "\n",
    "mistral_loader = torch.utils.data.DataLoader(mistral_data,\n",
    "                                             batch_size=2,\n",
    "                                             shuffle=False,\n",
    "                                             collate_fn=collate_fn(mistral_tokenizer))\n",
    "\n",
    "llama_loader = torch.utils.data.DataLoader(llama_data,\n",
    "                                          batch_size=2,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn(llama_tokenizer))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHHmwvA7Bp8r"
   },
   "source": [
    "Now we can use the models to generate responses, storing them in two separate lists and visualizing the distribution of outputed classes from the models. We can already notice that the mistral model heavily prefers the *animosity* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxsgSjZqwvb3",
    "outputId": "35035f35-b38a-447f-fd70-4461295afb67"
   },
   "outputs": [],
   "source": [
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "mistral_answers = generate_responses_batched(mistral_model, mistral_tokenizer, mistral_loader, 'mistral')\n",
    "llama_answers = generate_responses_batched(llama_model, llama_tokenizer, llama_loader, 'llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTv5dWPlXv72",
    "outputId": "dc3bf470-294c-478d-e996-f8a4dab2d5df"
   },
   "outputs": [],
   "source": [
    "print(np.unique(llama_answers, return_counts=True))\n",
    "print(np.unique(mistral_answers, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4qsOWlXCPdD"
   },
   "source": [
    "We performed a simple comparison of answers looking computing the percentage of disagreement between the two models, finding that it's pretty high: 51.33%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "1qFiPDCwWWUC",
    "outputId": "02c234e3-dd6f-4bf7-8c5d-32b3791581fb"
   },
   "outputs": [],
   "source": [
    "answers = pd.DataFrame({\n",
    "            \"True Answer\": df['label_category'],\n",
    "            \"Mistral\": mistral_answers,\n",
    "            \"Llama\": llama_answers\n",
    "          })\n",
    "\n",
    "print(f\"Percentage of disagreements between the models: {((answers['Mistral'] != answers['Llama']).sum() / 3):.2f}%\")\n",
    "disagreeing_answers = answers[answers['Mistral'] != answers['Llama']]\n",
    "disagreeing_answers.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0lXAEc7Co-6"
   },
   "source": [
    "In order to save computing resources we save in a persistent way, as a json file, the answers, so that they can be used without reloading and running the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSn3bmwrU9C0",
    "outputId": "28c44497-61fb-4d34-89e9-303c3803100d"
   },
   "outputs": [],
   "source": [
    "def save_answers(answers1, answers2, model_names = ['Mistral', 'Llama'], filename='zero_shot_answers.json'):\n",
    "    drive.mount('/content/drive')\n",
    "    path = f'/content/drive/MyDrive/NLP_tutorials/{filename}'\n",
    "\n",
    "    data = {model_names[0]: answers1, model_names[1]: answers2}\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "save_answers(mistral_answers, llama_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKcQ0ZfO2Fya"
   },
   "source": [
    "## Notes\n",
    "\n",
    "1. According to our tests, it should take you ~10 mins to perform full inference on 300 samples on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyZ8WU09zz-a"
   },
   "source": [
    "# [Task 4 - 0.5 points] Metrics\n",
    "\n",
    "In order to evaluate selected LLMs, we need to compute performance metrics.\n",
    "\n",
    "We compute **macro F1-score** and the ratio of failed responses generated by models (**fail-ratio**).\n",
    "\n",
    "That is, how frequent the LLM fails to follow instructions and provides incorrect responses that do not address the classification task.\n",
    "\n",
    "In summary, we parse generated responses as follows:\n",
    "- **0** if 'not-sexist'\n",
    "- **1** if 'threats'\n",
    "- **2** if 'derogation'\n",
    "- **3** if 'animosity'\n",
    "- **4** if 'prejudiced'\n",
    "- **0** if the model does not answer in either way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6lu64o80iX4"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 4 points, we require you to:\n",
    "\n",
    "* Write a ``compute_metrics`` function as the one reported below.\n",
    "* Compute metrics for the two selected LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myhRlKqsRksX"
   },
   "source": [
    "### Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Fmcw_9v0k9D"
   },
   "outputs": [],
   "source": [
    "def safe_process_response(pred):\n",
    "  '''\n",
    "  This function is responsible to guarantee that every llm's predictions that is\n",
    "  not the class label is mapped to 0, as recquired.\n",
    "\n",
    "  Inputs:\n",
    "    pred: prediction of the model, the string answer\n",
    "\n",
    "  Outputs:\n",
    "    integer corresponding to the predicted category\n",
    "  '''\n",
    "  try:\n",
    "    return process_response(pred)\n",
    "  except:\n",
    "    return 0\n",
    "\n",
    "def compute_metrics(y_pred, y_true):\n",
    "  \"\"\"\n",
    "    This function takes predicted and ground-truth labels and compute\n",
    "    metrics. In particular, this function compute accuracy and\n",
    "    fail-ratio metrics. This function internally invokes\n",
    "    `process_response` to compute metrics.\n",
    "\n",
    "    Inputs:\n",
    "      y_pred: parsed LLM responses\n",
    "      y_true: ground-truth binary labels\n",
    "\n",
    "    Outputs:\n",
    "      dictionary containing desired metrics\n",
    "  \"\"\"\n",
    "  f1_score = evaluate.load('f1')\n",
    "  acc_score = evaluate.load('accuracy')\n",
    "\n",
    "  y_pred = [safe_process_response(pred) for pred in y_pred]\n",
    "\n",
    "  y_pred = np.array(y_pred)\n",
    "  y_true = np.array(y_true)\n",
    "\n",
    "  f1 = f1_score.compute(predictions=y_pred, references=y_true, average='macro')\n",
    "  acc = acc_score.compute(predictions=y_pred, references=y_true)\n",
    "  fail_ratio = 1 - acc['accuracy']\n",
    "\n",
    "  return {**f1, **{'fail ratio': fail_ratio}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCyMV0XDEVc4"
   },
   "source": [
    "Below we defined the function that loads the lists of answers produced by the models. Those answers we saved in the Inference section. After it the metrics are computed and visualized. We can see that the Llama model outperfroms by a significant margin the Mistral one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyb0BIldVdnI"
   },
   "outputs": [],
   "source": [
    "def load_answers(filename='zero_shot_answers.json'):\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    path = f'/content/drive/MyDrive/NLP_tutorials/{filename}'\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return data[\"Mistral\"], data[\"Llama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "a072155d708542b0a03ef0bb2263c017",
      "e2a29c6902814fceb6901e52a4b85631",
      "ba11c8eb0b1c4f029b8f37d65dd780e1",
      "ffb9197b91b043b2ab5b164c5659e468",
      "c91bb59839014466a33636b67465c2d1",
      "ad2f34d683d34aa8b0ac7be8f85fdb5f",
      "e391cc6e2db74ff5b9d209b773382062",
      "3ed5a86626714122a237e4a30b6a01ed",
      "4436da0c0bbf46c2bc3ab65a4e5b2b38",
      "943bc035700541a1a744b80132d5a8b1",
      "d40e05f082f1456c81f258bc89682a63",
      "dcecf15fad6b4794a1a5f799b1549789",
      "15c517835f194d35aade0538caf1c7d0",
      "85ef99918efa404ba7aae4805a18a35d",
      "21c46c5280304d0591e89720af58e905",
      "3daa087a40f748caa2f75098570b5118",
      "4081b399f6eb48d6b504bad07c8908ab",
      "bc19c75df9bc46daa9846b393a9f7ede",
      "cfdd73ad7f764d28af73bcb62bfe63b6",
      "2ff012d8f3634b22abb108686a6c42ea",
      "aa0e2ea926f94d34b2a30367fdccb7f7",
      "2631860c36ab474cb27c7fbb7898a85e"
     ]
    },
    "id": "KJwAnfpa2v5J",
    "outputId": "9f9327c0-e03c-40a7-fe57-7ef4b0cc075f"
   },
   "outputs": [],
   "source": [
    "mistral_answers, llama_answers = load_answers()\n",
    "mistral_metrics = compute_metrics(mistral_answers, df['label'].values)\n",
    "llama_metrics = compute_metrics(llama_answers, df['label'].values)\n",
    "results = {\n",
    "    \"Model\": [\"Mistral\", \"Llama\"],\n",
    "    \"F1-score\": [mistral_metrics['f1'], llama_metrics['f1']],\n",
    "    \"Fail Ratio\": [mistral_metrics['fail ratio'], llama_metrics['fail ratio']]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='fancy_grid', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHyvV4QD2vZS"
   },
   "source": [
    "# [Task 5 - 1.0 points] Few-shot Inference\n",
    "\n",
    "So far, we have tested models in a zero-shot fashion: we provide the input text to classify and instruct the model to generate a response.\n",
    "\n",
    "We are now interested in performing few-shot prompting to see the impact of providing demonstration examples.\n",
    "\n",
    "To do so, we slightly change the prompt template as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEqsOHz63ReW"
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sexism detection.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Your task is to classify input text as not-sexist\n",
    "         or sexist. If sexist, classify input text according to one\n",
    "         of the following four categories: threats, derogation,\n",
    "         animosity, prejudiced discussion.\n",
    "\n",
    "         Below you find sexist categories definitions:\n",
    "         Threats: the text expresses intent or desire to harm a woman.\n",
    "         Derogation: the text describes a woman in a derogative manner.\n",
    "         Animosity: the text contains slurs or insults towards a woman.\n",
    "         Prejudiced discussion: the text expresses supports for\n",
    "         mistreatment of women as individuals.\n",
    "\n",
    "         Respond only by writing one of the following categories:\n",
    "         not-sexist, threats, derogation, animosity, prejudiced.\n",
    "\n",
    "        EXAMPLES: {examples}\n",
    "\n",
    "        TEXT: {text}\n",
    "\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SnaxuwN3ySF"
   },
   "source": [
    "The new prompt template reports some demonstration examples to instruct the model.\n",
    "\n",
    "Generally, we provide an equal number of demonstrations per class as shown in the example below. We have commented out this line in order to avoid overriding the correct prompt when running everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mNmRMQs4VCn"
   },
   "outputs": [],
   "source": [
    "# prompt = [\n",
    "#     {\n",
    "#         'role': 'system',\n",
    "#         'content': 'You are an annotator for sexism detection.'\n",
    "#     },\n",
    "#     {\n",
    "#         'role': 'user',\n",
    "#         'content': \"\"\"Your task is to classify input text as not-sexist\n",
    "#          or sexist. If sexist, classify input text according to one\n",
    "#          of the following four categories: threats, derogation,\n",
    "#          animosity, prejudiced discussion.\n",
    "\n",
    "#          Below you find sexist categories definitions:\n",
    "#          Threats: the text expresses intent or desire to harm a woman.\n",
    "#          Derogation: the text describes a woman in a derogative manner.\n",
    "#          Animosity: the text contains slurs or insults towards a woman.\n",
    "#          Prejudiced discussion: the text expresses supports for\n",
    "#          mistreatment of women as individuals.\n",
    "\n",
    "#          Respond only by writing one of the following categories:\n",
    "#          not-sexist, threats, derogation, animosity, prejudiced.\n",
    "\n",
    "#          EXAMPLES:\n",
    "#          TEXT: **example 1**\n",
    "#          ANSWER: threats\n",
    "#          TEXT: **example 2**\n",
    "#          ANSWER: not-sexist\n",
    "\n",
    "#          TEXT: {text}\n",
    "\n",
    "#         ANSWER:\n",
    "#         \"\"\"\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE0XnaVr3CZz"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "In order to get Task 5 points, we require you to:\n",
    "\n",
    "- Load ``demonstrations.csv`` and encode it into a ``pandas.DataFrame`` object.\n",
    "- Define a ``build_few_shot_demonstrations`` function as the one reported below.\n",
    "- Modify ``prepare_prompts`` to support demonstrations.\n",
    "- Perform few-shot inference as in Task 3.\n",
    "- Compute metrics as in Task 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPlyxfy3RdX3"
   },
   "source": [
    "### Our implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGMgvxIiFS51"
   },
   "source": [
    "Similarly as before we can see that this dataset is also perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "j81jOdwb6c4r",
    "outputId": "9bff69ab-3a02-4d5c-fd88-b4bec032eb5b"
   },
   "outputs": [],
   "source": [
    "demonstrations = pd.read_csv(f\"{data_path}/demonstrations.csv\")\n",
    "\n",
    "print(demonstrations['label_category'].value_counts())\n",
    "print(demonstrations['label_sexist'].value_counts())\n",
    "demonstrations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4ALcGjNFn8J"
   },
   "source": [
    "For the few-shot inference we have developed two different pipelines. The first one is the simple implementation that statically adds the demonstrations to the prompt while the second includes those examples that are deemed to be the most similar to the text we want to classify.\n",
    "\n",
    "We can thus, estimate the importance of carefully deciding which examples to include in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBl2YW1Fdmqz"
   },
   "source": [
    "#### Simple implementation\n",
    "\n",
    "In this section we implement few-shot inference in a very simple way. The examples to include in the prompt are sampled without considering the current text sample. We randomly choose `num_per_class` demonstration data for every class and add them to the prompt *examples* section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oyxypUG3ltH"
   },
   "outputs": [],
   "source": [
    "def build_few_shot_demonstrations(demonstrations, num_per_class=2):\n",
    "  \"\"\"\n",
    "    Inputs:\n",
    "      demonstrations: DataFrame wrapping demonstrations.csv\n",
    "      num_per_class: number of demonstrations per class\n",
    "\n",
    "    Outputs:\n",
    "      list of demonstrations to inject into the prompt template.\n",
    "  \"\"\"\n",
    "  sampled = (demonstrations\n",
    "             .groupby('label_category')\n",
    "             .sample(n=num_per_class)\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "  examples = [(row['text'], row['label_category']) for _, row in sampled.iterrows()]\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m9kW-KrZ7yM"
   },
   "outputs": [],
   "source": [
    "def prepare_prompts(texts, examples, prompt_template):\n",
    "  \"\"\"\n",
    "    This function format input text samples into instructions prompts.\n",
    "\n",
    "    Inputs:\n",
    "      texts: input texts to classify via prompting\n",
    "      examples: list of tuples - demonstrations to inject into the prompt template\n",
    "      prompt_template: the prompt template provided in this section of the assignments\n",
    "\n",
    "    Outputs:\n",
    "      input texts to classify in the form of instruction prompts\n",
    "  \"\"\"\n",
    "\n",
    "  examples = '\\n'.join([f\"TEXT:{text}\\n ANSWER: {label}\"\n",
    "                        for (text, label) in examples])\n",
    "  formatted_prompts = []\n",
    "  for text in texts:\n",
    "    formatted_prompts.append(prompt_template.format(text=text, examples=examples))\n",
    "\n",
    "  return formatted_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivB_Vs5DHBnk"
   },
   "source": [
    "Below we visualize the generated prompts one adding the demonstrations and the text to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etrnn5C4kRLt",
    "outputId": "e7c6cce7-db12-4c69-c234-9446bad04b3c"
   },
   "outputs": [],
   "source": [
    "demonstrations_list = build_few_shot_demonstrations(demonstrations, num_per_class=2)\n",
    "\n",
    "copied = df.copy()\n",
    "\n",
    "# Create new columns in the dataframe for the framed texts\n",
    "copied['few_shot_mistral'] = prepare_prompts(df['text'].values, demonstrations_list, mistral_prompt)\n",
    "copied['few_shot_llama'] = prepare_prompts(df['text'].values, demonstrations_list, llama_prompt)\n",
    "\n",
    "print(copied['few_shot_mistral'][0])\n",
    "print(copied['few_shot_llama'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ZjBR2VHmWL"
   },
   "source": [
    "In the following snippets we define a function that performs the inference and computes metrics for the provided models and for different `num_per_class` demonstrations and then we execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u26lgTPd47T4"
   },
   "outputs": [],
   "source": [
    "def compare_few_shots(models, tokenizers, prompt_template, text_data, demonstration_data, num_per_class=2):\n",
    "\n",
    "  '''\n",
    "  The function performs inference of the models (for different number of examples per class\n",
    "  on the provided data outputing metrices and model's answers.\n",
    "\n",
    "  Inputs:\n",
    "    models: list of models to run inference on\n",
    "    tokenizers: list of the tokenizers corresponding to the models\n",
    "    prompt_template: list of prompt templates provided in this assignment already tokenized with apply_chat_template\n",
    "    text_data: DataFrame containing the text to classify\n",
    "    demonstration_data: DataFrame containing the possible demonstrations\n",
    "    num_per_class: number of demonstrations per class, loop on that\n",
    "\n",
    "  Output:\n",
    "    Tuple containing a dictionary with computed metrics and a dictionary with model's answers\n",
    "  '''\n",
    "\n",
    "  results = {}\n",
    "  answers = {}\n",
    "\n",
    "  if not isinstance(num_per_class, list):\n",
    "    num_per_class = [num_per_class]\n",
    "\n",
    "  for num in num_per_class:\n",
    "    print(f\"Building demonstrations with number per class: {num}\")\n",
    "    demonstrations = build_few_shot_demonstrations(demonstration_data, num)\n",
    "    model_name = 'Mistral'\n",
    "    for i, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
    "      if i == 1:\n",
    "        model_name = 'Llama'\n",
    "      print(f\"\\t...Generating responses for {model_name}...\")\n",
    "      prompted_examples = prepare_prompts(text_data['text'].values, demonstrations, prompt_template[i])\n",
    "      prompted_data = Dataset.from_pandas(pd.DataFrame({'text':prompted_examples, 'label': text_data['label'].values}))\n",
    "      prompted_loader = torch.utils.data.DataLoader(prompted_data,\n",
    "                                          batch_size=2,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn(tokenizer))\n",
    "      response = generate_responses_batched(model, tokenizer, prompted_loader, model_name)\n",
    "      print(f\"\\t...Computing metrics for {model_name}...\")\n",
    "      metrics = compute_metrics(response, text_data['label'].values)\n",
    "      results[f'{model_name} {num}'] = metrics\n",
    "      answers[f'{model_name} {num}'] = response\n",
    "\n",
    "  return results, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IljfaXVL9rS3",
    "outputId": "22be7549-ce28-44bb-f2d2-fecb44db87c9"
   },
   "outputs": [],
   "source": [
    "# Create the two prompts with apply_chat_template\n",
    "mistral_prompt = mistral_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "llama_prompt = llama_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Call the function to evaluate all models\n",
    "results, answers = compare_few_shots([mistral_model, llama_model],\n",
    "                                     [mistral_tokenizer, llama_tokenizer],\n",
    "                                     [mistral_prompt, llama_prompt],\n",
    "                                     df,\n",
    "                                     demonstrations,\n",
    "                                     [2,4,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sNmRB2WH5WH"
   },
   "source": [
    "Similarly as before also in this case we save the answers and the results in a json file, facilitating the exploration of the results without the need of re-running everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miRbXXdib_sm",
    "outputId": "f266a6b9-d7c9-4f54-ea95-220788b2bdd2"
   },
   "outputs": [],
   "source": [
    "def save_json(results, answers, filename='simple_few_shot_results.json'):\n",
    "\n",
    "  drive.mount('/content/drive')\n",
    "  path = f'/content/drive/MyDrive/NLP_tutorials/{filename}'\n",
    "  with open(path, 'w') as f:\n",
    "    json.dump({'results': results, 'answers': answers}, f, indent=2)\n",
    "\n",
    "save_json(results, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne7u8Tdidu5p"
   },
   "source": [
    "#### Similarity based\n",
    "\n",
    "In the following section we implement few-shot inference considering the cosine similarity between the embeddings of the demonstration examples and the text we are interested in classifying. The most similar demonstrations for each class are then selected and used in the prompt.\n",
    "\n",
    "For the first thing we load the encoder that will be used to compute embeddings of the demonstration examples. We chose the `BAAI/bge-large-en-v1.5` encoder. We didn't use the internal representations of the two models used to predict classes to improve on efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "96cdf774627a49c584228f2e443ffdae",
      "b0d3a171f51c4e06a75ba3820397aa91",
      "de4a47475dff4e83aecb4a945346e68d",
      "5e89ee5a1e00475db898b17f7480b4ec",
      "b3923ee1e5e34fe88d02b3decc579aae",
      "edbb3b1b00954cc6b49f995a7f3f6842",
      "f96dad5f1fbe4049959e7221a806ed98",
      "cf42fc728dbc4ff2bef6e3f19521071f",
      "a5c76a28c6d74ac6bb13f4f1fee2e336",
      "a0af6b9fb6954d1e9e02822ba4a7126a",
      "db38230401784cb28f286aebe8998a0f",
      "99b13f26b3be4b6d884b2e19ce8a7ea7",
      "eaf1e7f86ea9440496175f54f17964b3",
      "77db7c0e0a87450ab13463e14a2ae44b",
      "9aff742b00e1477ba96f66b5bd4ceaf0",
      "cb51d9c83abb44c59ebdd60904642139",
      "7b0155f45c07475b91273db062542f2f",
      "b30182c0f92e497b908c6348a5f16529",
      "4d3bd134b64d416c897f9dd95d59f6c0",
      "89428979700b41a1908403a836eb6cd3",
      "ada124965ddb4e45bd5d7836f327fb7c",
      "15c21920eacf4dc09f79fa3f07bf8c89",
      "21eab3a495f7434d8350fc08cbc86fde",
      "f1e75aed187f4dd99611f3695cdf840d",
      "5ec6fa407a5341cfa69e33fcde68c190",
      "4207b80fd0dd47b3b84c3ba75e2cdac0",
      "97a5a65fbedf470f9c4867ba6da03520",
      "47461c3c9a5c4d9f998aeee9ca673a21",
      "94a45dce23f54721b4bbc0b8f72d5c22",
      "53bc97a6c1414af38f60722d3c612d36",
      "159ca34e52f643bd96b5043c835818d9",
      "088147d7211847b58ca63c69edac6505",
      "e89534564a034c4a8129cc6de17f1d1b",
      "1ae39ad80af34bb68af971beb55bb69a",
      "dea1a8c49e32455d9a3e38910949e9a4",
      "d7f791aba67f478c844a70136262f005",
      "c6f79c9609654038a76c729783d65312",
      "9b862f0effad4801a5a2ef320a5c303b",
      "879abcd976c846a2bf0e2ddb90b901f1",
      "1dfc73ef440242b9978bbb44d8f4c5c2",
      "e7fac04adbd94bdd8aa689747caf3866",
      "0d12ee615a024826977e440633c922cb",
      "f979833019e24af98f8d233f110ec474",
      "a847265d2ce74230878b974e1d4d2c22",
      "ad5eebdf7a8d4180b6fc562acf3e98de",
      "18e87fea321b45618968f97b63ad8786",
      "9aa35e3448d04158924891a687fe1d31",
      "fe38bf9d1a114e9e9397a6ab4e99e190",
      "fb515ac959354d6f84f0fa808fa87d27",
      "5e66c2497ba6485d810cfc42d201289d",
      "317cf07f219041cc9466fc333d7c381c",
      "fa0474f9a1a140c9b63511bdc4792414",
      "487c735981554ab2bb688de477b04505",
      "0a1b141010e64a17a45c1ab8bdf9bcbf",
      "0fa29d99ded24c25aad1c9d4aaad4b52",
      "c4387a2dfe114b28913e462d8d06bb0d",
      "274c176a37a8408688315ba0a189c24f",
      "5dabd9139cbc4cd7b8839c625a02c230",
      "018825f4efec4f52a81fe38560d90d3e",
      "ed0f59cc23644e609964f7a730d9b838",
      "a7b5819dd0e34eaab71d46d93199287f",
      "0e270545275e431fa39a46db9f23a1e0",
      "e5ae58192200463daeba3296b4181cd6",
      "0765193c6b074da8a8f714a7f14f46bc",
      "7e9b0e9f4a8f43a39b007cf4cc51fcf7",
      "35ae4b7522c040acb858b486d117d88f",
      "fb50ed21c0864be5aac4f73439c2cf94",
      "989b6128b3154c51bbabf0ce2b11b4ce",
      "3c34d7d975004cd39cff97f3dec7e675",
      "fc756879500d4e40b783ab8d5373aaf6",
      "e4bde3e0e49e41748ac54280fb64d8f5",
      "f82d55704f2d4098a22a64ee06783648",
      "3b0e4f9d5280480998d5761c5051e5a8",
      "c735890cd594439f99f1daf692280c65",
      "3f258b1c2c2a476ba1ab2bfbce285998",
      "6188a4ca267b4e58be0f490d7040028a",
      "8cc6e83dc1c2421a9317b0137544370a",
      "492da1d553624b6bbdb1fff53ef863ce",
      "6ddf4a391bb548b192964c980d6c2b72",
      "f123e0edb73d4d2aa3b1415148e76885",
      "997ade4cd0b94ae9b036fd71701281f8",
      "3eed458eebcf46a1ad188261c9cc2c72",
      "3d86adbc54734239969bfe08e57c4919",
      "08fac3df2fb64e4e82e20bfe9cf62b62",
      "e9d93256029a47178bb9f8461e7740b9",
      "0335b8260fad49d5939671420186eca4",
      "85f711408e6d4d09a847a8420a71414b",
      "74799587c89a4e93adc4bbb53d1ce0c5",
      "d9189def33734413af573bb9fd90beaf",
      "c31af36f3f7c4901bc2984644bcc1c8f",
      "1c0dd9bfca3a44f699bd8bcb43cb5c62",
      "6bf0a9f50ef0445db4e49c3fc20e3230",
      "992872b0bbde46b9bf41c2fd50b3ab48",
      "6ad5f133f4aa4274a86627db450ad02f",
      "14778d09af6a43a086e6e21ec4214c37",
      "9069d5d620cf42b39704edf6ab751c96",
      "af8a03fbf607443582b177b54beea41d",
      "fa01a7b1f6334cf9b1be38ed82238706",
      "2ad4692b3fba431daf3edb048a46f648",
      "be383cf6cfdc43719beb54486e4c845b",
      "30c0312c722a4b90b6dd0ea7dde23151",
      "c5e2352627bb4317a4c6c47c512fe14b",
      "5fdfdf8dee3445008a56a0a0ffef971c",
      "54570cb52ddd4f11b8aa36026b9c06fe",
      "51240998e4ba485f96aef00576bcfab2",
      "f34f8df3ee40413788ecad1bbf98e8be",
      "eff0e921705f4c198338e6bfa7eb3f64",
      "fa58dd3d5f26406384abe5c4cd931b53",
      "f20690acc37e44b6a8451687495d9af5",
      "e6983fb41daf48f6984f95027fb1e7b2",
      "190efbe890084d4bbc2c455018e9e8bd",
      "f87d713edf79487bab6d7e3d81ef89cf",
      "289f332f05044cd8b66f626c22352c45",
      "864a7de7ca2142c3b65730d72421d164",
      "f8e68274d45f4432816fd8012cd82668",
      "3ac6d185b9184fc9b0e47ca174c76fd5",
      "bce373afb8a74390a0d19b8f456dd564",
      "64905cedaab8425e905f5541735779c9",
      "bcc5b43f69e94c1b91ebe630d8996d2f",
      "63da66a0766e4171a3a83d1a42df65af",
      "988a9f0743cf4b16833cc84a38f6438b"
     ]
    },
    "id": "Yk5DMF2ElXte",
    "outputId": "b5348c05-c42c-4f2a-b400-66246787fedb"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "encoder_card = 'BAAI/bge-large-en-v1.5'\n",
    "encoder_model = SentenceTransformer(encoder_card)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrNbwgwchOTB"
   },
   "source": [
    "The following code snippet computes the embeddings of the every example in the demonstration dataset. The lines are commented out because we have already saved it in a persistent way.\n",
    "\n",
    "To recompute it uncomment the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "308659166db54f2a94e8218e46d034f0",
      "149cacc410fa473385e459fd991aa0d7",
      "06c9c5f7da9d4b8395f763c091700b92",
      "89375f0ac7de4ac9aaf0a77d888ff92c",
      "a2d738592d4d4e40bcba28781ed305c6",
      "8e701894323b46159afe01f2892040f0",
      "aabfe0ce3ab34fcb8eb6b28c269090eb",
      "5d46e895ae444aac8f1e48905af134c6",
      "8403f12e219c40ca9e04d5736ac169a0",
      "9cea0ef6dc9c405ab048cf716717eb2d",
      "562d5c9cab6c4b0bb247259060c8b1a4"
     ]
    },
    "id": "fEHaT4bklbQV",
    "outputId": "e68c7ee2-eb37-4553-fd8f-c2a16d123992"
   },
   "outputs": [],
   "source": [
    "# # Precompute the embeddings and store them\n",
    "# embeddings = encoder_model.encode(demonstrations['text'].values,\n",
    "#                                   batch_size=32,\n",
    "#                                   convert_to_tensor=True,\n",
    "#                                   normalize_embeddings=True,\n",
    "#                                   show_progress_bar=True)\n",
    "\n",
    "# def save_embeddings(embeddings, filename='embeddings.npy'):\n",
    "#   drive.mount('/content/drive')\n",
    "#   path = f'/content/drive/MyDrive/NLP_tutorials/{filename}'\n",
    "#   # Corrected arguments order and conversion to numpy array for np.save\n",
    "#   np.save(path, embeddings.cpu().numpy())\n",
    "\n",
    "# save_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A7XJw0Rk_xf"
   },
   "source": [
    "With the following utility function we inspect the most and less similar examples for some text samples. We can in fact notice that some similarity between the texts is present:\n",
    "- the second example is paired with a text about shooting the girl, which is exactly what the text is about.\n",
    "- the third example is paired with a text about weight and fatness which is also a topic in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-k7mH1XlCyJ"
   },
   "outputs": [],
   "source": [
    "def report_similarity(\n",
    "    model: SentenceTransformer,\n",
    "    corpus_texts: list[str],\n",
    "    corpus_embeddings: np.ndarray,\n",
    "    query_texts: list[str],\n",
    "    top_k: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Prints the most similar and least similar texts from a corpus\n",
    "    for each query text.\n",
    "\n",
    "    Args:\n",
    "        model: A SentenceTransformer instance.\n",
    "        corpus_texts: List of texts corresponding to precomputed embeddings.\n",
    "        corpus_embeddings: np.ndarray of shape (N, D)\n",
    "        query_texts: List of new texts to compare to the corpus.\n",
    "        top_k: Number of most-similar results to show (default = 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute embeddings for the new query samples\n",
    "    query_embeddings = model.encode(query_texts, convert_to_tensor=True)\n",
    "\n",
    "    # Convert corpus to tensor if not already\n",
    "    corpus_tensor = torch.tensor(corpus_embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    for query, q_emb in zip(query_texts, query_embeddings):\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        similarities = (corpus_tensor @ q_emb).cpu().numpy()\n",
    "\n",
    "        # Most similar\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        for rank, idx in enumerate(top_indices, start=1):\n",
    "            print(f\"\\nTOP {rank} MOST SIMILAR:\")\n",
    "            print(f\"Text: {corpus_texts[idx]}\")\n",
    "            print(f\"Score: {similarities[idx]:.4f}\")\n",
    "\n",
    "        # Least similar\n",
    "        worst_idx = np.argmin(similarities)\n",
    "        print(\"\\nLEAST SIMILAR:\")\n",
    "        print(f\"Text: {corpus_texts[worst_idx]}\")\n",
    "        print(f\"Score: {similarities[worst_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e6Yn-k7m4rX"
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(f\"/content/drive/MyDrive/NLP_tutorials/embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSUp7UIWmoCU",
    "outputId": "7647b111-852f-4652-8bec-5ab2e0c4beb5"
   },
   "outputs": [],
   "source": [
    "query_texts = df[\"text\"].sample(n=3, random_state=42).tolist()\n",
    "report_similarity(encoder_model, demonstrations['text'].values, embeddings, query_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkUTADYuKbXB"
   },
   "source": [
    "The remaining parts of this sub-section mirror what we did in the previous sub-section. We implement:\n",
    "- function to extract the correct examples to add to the prompt;\n",
    "- function to prepare prompts;\n",
    "- function that executes the models for different values of `num_per_class` and computes the metrics.\n",
    "\n",
    "We finally call the last function and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nmCWTO6dzbO"
   },
   "outputs": [],
   "source": [
    "def build_similarity_few_shot_demonstration(text, demonstrations, embeddings, num_per_class=2):\n",
    "\n",
    "  \"\"\"\n",
    "  The function builds the demonstrations to inject into the prompt template looking at the cosine similarity.\n",
    "\n",
    "    Inputs:\n",
    "      text: input text, with respect to which we should build the demonstrations\n",
    "      demonstrations: DataFrame wrapping demonstrations.csv\n",
    "      embeddings: numpy array with precomputed embeddings of the demonstrations.csv texts\n",
    "      num_per_class: number of demonstrations per class\n",
    "\n",
    "    Outputs:\n",
    "      list of demonstrations to inject into the prompt template.\n",
    "  \"\"\"\n",
    "\n",
    "  # Build the embedding of the text\n",
    "  text_embedding = encoder_model.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "  # Compute cosine similarity with the precomputed embeddings\n",
    "  # Move emb_tensor to the same device as text_embedding\n",
    "  emb_tensor = torch.tensor(embeddings, dtype=torch.float32).to(text_embedding.device)\n",
    "  similarities = (emb_tensor @ text_embedding).cpu().numpy()\n",
    "\n",
    "  # Choose num_per_class most similar examples for every class\n",
    "  demonstrations = demonstrations.copy()\n",
    "  demonstrations[\"similarity\"] = similarities\n",
    "\n",
    "  # Return the list of selected prompts\n",
    "  selected = (demonstrations\n",
    "              .sort_values(\"similarity\", ascending=False)\n",
    "              .groupby(\"label_category\")\n",
    "              .head(num_per_class))\n",
    "\n",
    "  examples = [(row['text'], row['label_category']) for _, row in selected.iterrows()]\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0v6aHVy_r6v"
   },
   "outputs": [],
   "source": [
    "def similarity_prepare_prompts(texts, examples, prompt_template, embeddings, num_per_class=2):\n",
    "  \"\"\"\n",
    "    This function format input text samples into instructions prompts.\n",
    "\n",
    "    Inputs:\n",
    "      texts: input texts to classify via prompting\n",
    "      examples: dataframe that warps demonstrations.csv\n",
    "      prompt_template: the prompt template provided in this assignment\n",
    "      embeddings: numpy array with precomputed embeddings of the demonstrations.csv texts\n",
    "\n",
    "    Outputs:\n",
    "      input texts to classify in the form of instruction prompts\n",
    "  \"\"\"\n",
    "  formatted_prompts = []\n",
    "  for text in texts:\n",
    "    examples_for_text = build_similarity_few_shot_demonstration(text,\n",
    "                                                                 examples,\n",
    "                                                                 embeddings,\n",
    "                                                                 num_per_class=num_per_class)\n",
    "    examples_for_text = '\\n'.join([f\"TEXT:{text}\\n ANSWER: {label}\"\n",
    "                                  for (text, label) in examples_for_text])\n",
    "    formatted_prompts.append(prompt_template.format(text=text, examples=examples_for_text))\n",
    "\n",
    "  return formatted_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MlCR8KCBvDc"
   },
   "outputs": [],
   "source": [
    "def compare_similarity_few_shots(models, tokenizers, prompt_template, text_data, demonstration_data, embeddings, num_per_class=2):\n",
    "\n",
    "  '''\n",
    "  The function performs inference of the models (for different number of examples per class\n",
    "  on the provided data outputing metrices and model's answers.\n",
    "\n",
    "  Inputs:\n",
    "    models: list of models to run inference on\n",
    "    tokenizers: list of the tokenizers corresponding to the models\n",
    "    prompt_template: list of prompt templates provided in this assignment already tokenized with apply_chat_template\n",
    "    text_data: DataFrame containing the text to classify\n",
    "    demonstration_data: DataFrame containing the possible demonstrations\n",
    "    num_per_class: number of demonstrations per class, loop on that\n",
    "\n",
    "  Output:\n",
    "    dictionary with metrics\n",
    "    dictionary with model's answers\n",
    "  '''\n",
    "\n",
    "  results = {}\n",
    "  answers = {}\n",
    "\n",
    "  if not isinstance(num_per_class, list):\n",
    "    num_per_class = [num_per_class]\n",
    "\n",
    "  for num in num_per_class:\n",
    "    print(f\"Number of samples per class: {num}\")\n",
    "    model_name = 'Mistral'\n",
    "    for i, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
    "      if i == 1:\n",
    "        model_name = 'Llama'\n",
    "      print(f\"\\t...Generating responses for {model_name}...\")\n",
    "      prompted_examples = similarity_prepare_prompts(text_data['text'].values, demonstrations, prompt_template[i], embeddings, num)\n",
    "      prompted_data = Dataset.from_pandas(pd.DataFrame({'text':prompted_examples, 'label': text_data['label'].values}))\n",
    "      prompted_loader = torch.utils.data.DataLoader(prompted_data,\n",
    "                                          batch_size=2,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn(tokenizer))\n",
    "      response = generate_responses_batched(model, tokenizer, prompted_loader, model_name)\n",
    "      print(f\"\\t...Computing metrics for {model_name}...\")\n",
    "      metrics = compute_metrics(response, text_data['label'].values)\n",
    "      results[f'{model_name} {num}'] = metrics\n",
    "      answers[f'{model_name} {num}'] = response\n",
    "\n",
    "  return results, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjIiegyoCiUd",
    "outputId": "8b5d9db5-1c8d-439c-f1e2-a61362d85c27"
   },
   "outputs": [],
   "source": [
    "# Create the two prompts with apply_chat_template\n",
    "mistral_prompt = mistral_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "llama_prompt = llama_tokenizer.apply_chat_template(\n",
    "    prompt,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "# Call the function to evaluate all models\n",
    "results, answers = compare_similarity_few_shots([mistral_model, llama_model],\n",
    "                                     [mistral_tokenizer, llama_tokenizer],\n",
    "                                     [mistral_prompt, llama_prompt],\n",
    "                                     df,\n",
    "                                     demonstrations,\n",
    "                                     embeddings,\n",
    "                                     [2,4,6])\n",
    "\n",
    "save_json(results, answers, 'similarity_few_shot_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGpZWQb45XXK"
   },
   "source": [
    "## Notes\n",
    "\n",
    "1. You are free to pick any value for ``num_per_class``.\n",
    "\n",
    "2. According to our tests, few-shot prompting increases inference time by some minutes (we experimented with ``num_per_class`` $\\in [2, 4]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHuT1a1GLJnd"
   },
   "source": [
    "# [Task 6 - 1.0 points] Error Analysis\n",
    "\n",
    "We are now interested in evaluating model responses and comparing their performance.\n",
    "\n",
    "This analysis helps us in understanding\n",
    "\n",
    "- Classification task performance gap: are the models good at this task?\n",
    "- Generation quality: which kind of responses do models generate?\n",
    "- Errors: which kind of mistakes do models do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kjEAHD4LJne"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "In order to get Task 6 points, we require you to:\n",
    "\n",
    "* Compare classification performance of selected LLMs in a Table.\n",
    "* Compute confusion matrices for selected LLMs.\n",
    "* Briefly summarize your observations on generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0glgVdSCRQ8k"
   },
   "source": [
    "### Our implementation\n",
    "\n",
    "In this section we compare:\n",
    "- The models in zero-shot inference;\n",
    "- The models in few-shot inference;\n",
    "- The benefits of few-shot inference;\n",
    "- The two different strategies to include demonstrations in the prompt;\n",
    "- The effect of num_per_class on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilxhzcSSbh5"
   },
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJY_lzwDLsJP"
   },
   "outputs": [],
   "source": [
    "def load_json(filename='results.json'):\n",
    "\n",
    "    '''\n",
    "    This function is responsible for loading the results and answers we previously saved\n",
    "    '''\n",
    "\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    path = f'/content/drive/MyDrive/NLP_tutorials/{filename}'\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    results = data.get('results', {})\n",
    "    answers = data.get('answers', {})\n",
    "\n",
    "    return results, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-G_3juhNdNS"
   },
   "source": [
    "The three functions defined in the following code snippet are used to visualize the confusion matrices and the precomputed metrics of the models. Moreover, we also additionaly implement a function that computes per class precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqzZ8z53CYPh"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(answers, y_true, labels=None, normalize='true'):\n",
    "\n",
    "  for model_name, model_answers in answers.items():\n",
    "    cm = confusion_matrix(y_true, model_answers, labels=labels, normalize=normalize)\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    plt.title(f'{model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_confusion_matrix_two_sets(answers1, answers2, y_true, labels=None, titles=(\"Random\", \"Similarity\"), normalize='true'):\n",
    "\n",
    "  model_names = list(answers1.keys())\n",
    "\n",
    "  for model_name in model_names:\n",
    "    model_answers1 = answers1[model_name]\n",
    "    model_answers2 = answers2[model_name]\n",
    "    cm1 = confusion_matrix(y_true, model_answers1, labels=labels, normalize=normalize)\n",
    "    cm2 = confusion_matrix(y_true, model_answers2, labels=labels, normalize=normalize)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    # Left\n",
    "    sns.heatmap(\n",
    "            cm1,\n",
    "            annot=True,\n",
    "            cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            fmt='.2f',\n",
    "            ax=axes[0]\n",
    "        )\n",
    "    axes[0].set_title(f\"{model_name} — {titles[0]}\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True\")\n",
    "\n",
    "    # Right\n",
    "    sns.heatmap(\n",
    "            cm2,\n",
    "            annot=True,\n",
    "            cmap='Blues',\n",
    "            xticklabels=labels,\n",
    "            yticklabels=labels,\n",
    "            fmt='.2f',\n",
    "            ax=axes[1]\n",
    "        )\n",
    "    axes[1].set_title(f\"{model_name} — {titles[1]}\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def build_results(zeroA, zeroB, random_few_shot, similarity_few_shot):\n",
    "\n",
    "    results = {\n",
    "        \"Mistral\": {\n",
    "            \"zero_shot\": zeroA,\n",
    "            \"simple_few_shot\": {},\n",
    "            \"similarity_few_shot\": {}\n",
    "        },\n",
    "        \"Llama\": {\n",
    "            \"zero_shot\": zeroB,\n",
    "            \"simple_few_shot\": {},\n",
    "            \"similarity_few_shot\": {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Simple few-shot\n",
    "    for key, metrics in random_few_shot.items():\n",
    "        model_name, shots_str = key.split()\n",
    "        shots = int(shots_str)\n",
    "\n",
    "        results[model_name][\"simple_few_shot\"][shots] = metrics\n",
    "\n",
    "    # Similarity few-shot\n",
    "    for key, metrics in similarity_few_shot.items():\n",
    "        model_name, shots_str = key.split()\n",
    "        shots = int(shots_str)\n",
    "\n",
    "        results[model_name][\"similarity_few_shot\"][shots] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "def summarize_results(results):\n",
    "    \"\"\"\n",
    "    Create the comparison table:\n",
    "    Mode | Shots | Mistral F1 | Mistral Fail | Llama F1 | Llama Fail\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    rows.append({\n",
    "        \"Mode\": \"zero_shot\",\n",
    "        \"Shots\": 0,\n",
    "        \"Mistral F1\": results[\"Mistral\"][\"zero_shot\"][\"f1\"],\n",
    "        \"Mistral Fail\": results[\"Mistral\"][\"zero_shot\"][\"fail ratio\"],\n",
    "        \"Llama F1\": results[\"Llama\"][\"zero_shot\"][\"f1\"],\n",
    "        \"Llama Fail\": results[\"Llama\"][\"zero_shot\"][\"fail ratio\"],\n",
    "    })\n",
    "\n",
    "    for shots in sorted(results[\"Mistral\"][\"simple_few_shot\"].keys()):\n",
    "        rows.append({\n",
    "            \"Mode\": \"random_few_shot\",\n",
    "            \"Shots\": shots,\n",
    "            \"Mistral F1\": results[\"Mistral\"][\"simple_few_shot\"][shots][\"f1\"],\n",
    "            \"Mistral Fail\": results[\"Mistral\"][\"simple_few_shot\"][shots][\"fail ratio\"],\n",
    "            \"Llama F1\": results[\"Llama\"][\"simple_few_shot\"][shots][\"f1\"],\n",
    "            \"Llama Fail\": results[\"Llama\"][\"simple_few_shot\"][shots][\"fail ratio\"],\n",
    "        })\n",
    "\n",
    "    for shots in sorted(results[\"Mistral\"][\"similarity_few_shot\"].keys()):\n",
    "        rows.append({\n",
    "            \"Mode\": \"similarity_few_shot\",\n",
    "            \"Shots\": shots,\n",
    "            \"Mistral F1\": results[\"Mistral\"][\"similarity_few_shot\"][shots][\"f1\"],\n",
    "            \"Mistral Fail\": results[\"Mistral\"][\"similarity_few_shot\"][shots][\"fail ratio\"],\n",
    "            \"Llama F1\": results[\"Llama\"][\"similarity_few_shot\"][shots][\"f1\"],\n",
    "            \"Llama Fail\": results[\"Llama\"][\"similarity_few_shot\"][shots][\"fail ratio\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "    return df\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_per_class_precision_recall(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Compute per-class precision and recall only.\n",
    "\n",
    "    y_true: list of true labels (strings or ints)\n",
    "    y_pred: list of predicted labels (strings or ints)\n",
    "    classes: list of all class labels (strings)\n",
    "\n",
    "    Returns:\n",
    "    dict[class] = {\"precision\": ..., \"recall\": ...}\n",
    "    \"\"\"\n",
    "    safe_preds = [p if p in classes else None for p in y_pred]\n",
    "\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(\n",
    "        y_true, safe_preds, labels=classes, zero_division=0\n",
    "    )\n",
    "\n",
    "    per_class = {cls: {\"precision\": p, \"recall\": r} for cls, p, r in zip(classes, precision, recall)}\n",
    "    return per_class\n",
    "\n",
    "def build_results_per_class(df, zero_preds, few_shot_random, few_shot_similarity, classes):\n",
    "    \"\"\"\n",
    "    Compute per-class precision/recall for all configurations and store in a results dict.\n",
    "\n",
    "    Parameters:\n",
    "    - df: dataframe containing 'labels' (int) and 'label_category' (str)\n",
    "    - zero_preds: dict of zero-shot predictions, e.g. {\"Mistral\": [...], \"Llama\": [...]}\n",
    "    - few_shot_random: dict like {\"Mistral 2\": [...], \"Llama 6\": [...]} (pred strings)\n",
    "    - few_shot_similarity: dict like above\n",
    "    - classes: list of class strings\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # ----- zero-shot -----\n",
    "    for model, preds in zero_preds.items():\n",
    "        results[model] = {}\n",
    "        results[model][\"zero_shot\"] = {\n",
    "            \"per_class\": compute_per_class_precision_recall(\n",
    "                df['label_category'].tolist(), preds, classes\n",
    "            )\n",
    "        }\n",
    "\n",
    "    # ----- few-shot modes -----\n",
    "    for mode_name, few_shot_dict in [(\"random_few_shot\", few_shot_random),\n",
    "                                     (\"similarity_few_shot\", few_shot_similarity)]:\n",
    "        for key, preds in few_shot_dict.items():\n",
    "            model_name, shots_str = key.split()\n",
    "            shots = int(shots_str)\n",
    "\n",
    "            if model_name not in results:\n",
    "                results[model_name] = {}\n",
    "\n",
    "            results[model_name].setdefault(mode_name, {})\n",
    "\n",
    "            results[model_name][mode_name][shots] = {\n",
    "                \"per_class\": compute_per_class_precision_recall(\n",
    "                    df['label_category'].tolist(), preds, classes\n",
    "                )\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_delta_table(results, model_name, shots=6):\n",
    "    \"\"\"\n",
    "    Compute delta (6-shot - zero-shot) per class for both random and similarity few-shot.\n",
    "\n",
    "    Returns a DataFrame and prints nicely.\n",
    "    \"\"\"\n",
    "    zero = results[model_name][\"zero_shot\"][\"per_class\"]\n",
    "    rnd = results[model_name][\"random_few_shot\"][shots][\"per_class\"]\n",
    "    sim = results[model_name][\"similarity_few_shot\"][shots][\"per_class\"]\n",
    "\n",
    "    rows = []\n",
    "    for cls in zero.keys():\n",
    "        rows.append({\n",
    "            \"Class\": cls,\n",
    "            \"Δ Precision (Random)\": rnd[cls][\"precision\"] - zero[cls][\"precision\"],\n",
    "            \"Δ Recall (Random)\": rnd[cls][\"recall\"] - zero[cls][\"recall\"],\n",
    "            \"Δ Precision (Similarity)\": sim[cls][\"precision\"] - zero[cls][\"precision\"],\n",
    "            \"Δ Recall (Similarity)\": sim[cls][\"recall\"] - zero[cls][\"recall\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"\\n### {model_name} — Per-Class Improvements (Zero-Shot → {shots}-Shot)\\n\")\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ5YL-SvPJso"
   },
   "source": [
    "We now load the answers and results and prepare them for the final comparisons and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Igi3kOcdjQOU",
    "outputId": "eec97f75-b123-4243-f793-b7317f1e7e17"
   },
   "outputs": [],
   "source": [
    "results_similarity_few_shot, answers_similarity_few_shot = load_json('similarity_few_shot_results.json')\n",
    "results_simple_few_shot, answers_simple_few_shot = load_json('simple_few_shot_results.json')\n",
    "mistral_answers, llama_answers = load_answers()\n",
    "\n",
    "df['label'] = df.apply(lambda x: process_response(x['label_category']), axis=1)\n",
    "mistral_metrics = compute_metrics(mistral_answers, df['label'].values)\n",
    "llama_metrics = compute_metrics(llama_answers, df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jqtyOSVPVCX"
   },
   "source": [
    "#### Numerical metrics\n",
    "\n",
    "In this sub-section we present the metrics in a table form. We can conclude that:\n",
    "- Llama is better than Mistral in zero-shot inference;\n",
    "- The performance difference smoothes out when using few-shot prompting. Mistral can be seen to outperform Llama on some experimentation settings;\n",
    "- Using the similarity based approach in adding demonstrations significantly increases performance;\n",
    "- While we can see that adding demonstrations helps there are no evident patterns that suggest the best number of examples to add per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1UiVAJ80DAf",
    "outputId": "02cc64e2-6838-402c-c62e-5776b08b11f6"
   },
   "outputs": [],
   "source": [
    "results = build_results(mistral_metrics,\n",
    "                        llama_metrics,\n",
    "                        results_simple_few_shot,\n",
    "                        results_similarity_few_shot)\n",
    "results_df = summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsERiMp4Q6r6"
   },
   "source": [
    "Below we also look at precision and recall per class to understand how different models behave with different classes. And what classes benefit the most from including more examples.\n",
    "\n",
    "\n",
    "***Analize the output!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daJ5eYEG5_rg",
    "outputId": "11d1f3d4-25ac-48ec-ddfd-f98e607f6bc7"
   },
   "outputs": [],
   "source": [
    "classes = df['label_category'].unique().tolist()\n",
    "zero_preds = {\"Mistral\": mistral_answers, \"Llama\": llama_answers}\n",
    "results_per_class = build_results_per_class(df, zero_preds, answers_simple_few_shot, answers_similarity_few_shot, classes)\n",
    "# Compute delta tables\n",
    "df_mistral_delta = compute_delta_table(results_per_class, \"Mistral\", shots=6)\n",
    "df_llama_delta = compute_delta_table(results_per_class, \"Llama\", shots=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Od_elEPioU"
   },
   "source": [
    "#### Confusion matrices\n",
    "\n",
    "In the sub-section all the confusion matrices are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "PiIXKlP7tMog",
    "outputId": "e224afe0-4080-4f0e-f6f0-fd78c353b8bd"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix({'Mistral': mistral_answers, 'Llama': llama_answers}, df['label_category'].values, labels=df['label_category'].unique(), normalize='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DvI8ULls_ug",
    "outputId": "9e28f3ab-63b8-48d6-f8ac-e4ac6b1b9f8c"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix_two_sets(answers_simple_few_shot, answers_similarity_few_shot, df['label_category'].values, labels=df['label_category'].unique(), normalize='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7viuFvJAS0fb",
    "outputId": "34dc5323-730e-4832-90ee-435ab2a4bb3e"
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix_two_sets(answers_simple_few_shot, answers_similarity_few_shot, df['label_category'].values, labels=df['label_category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QWlVXJgLJne"
   },
   "source": [
    "# [Task 7 - 1.0 points] Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fsdV99TLJne"
   },
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-hUXYaLLJne"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "The report is not a copy-paste of graphs, tables, and command outputs.\n",
    "\n",
    "* Summarize classification performance in Table format.\n",
    "* **Do not** report command outputs or screenshots.\n",
    "* Report learning curves in Figure format.\n",
    "* The error analysis section should summarize your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fURV8zfPLJne"
   },
   "source": [
    "# Submission\n",
    "\n",
    "* **Submit** your report in PDF format.\n",
    "* **Submit** your python notebook.\n",
    "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn1tUeYzLJne"
   },
   "source": [
    "# FAQ\n",
    "\n",
    "Please check this frequently asked questions before contacting us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYAOVGvKhtTQ"
   },
   "source": [
    "### Model cards\n",
    "\n",
    "You can pick any open-source model card you like.\n",
    "\n",
    "We recommend starting from those reported in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWG72N-LLJne"
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Everything can be done via ``transformers`` APIs.\n",
    "\n",
    "However, you are free to test frameworks, such as [LangChain](https://www.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/) [LitParrot](https://github.com/awesome-software/lit-parrot), provided that you correctly address task instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTQpV6JeryU9"
   },
   "source": [
    "### Task Performance\n",
    "\n",
    "The task is challenging and zero-shot prompting may show relatively low performance depending on the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coy_pJ40LJne"
   },
   "source": [
    "### Prompt Template\n",
    "\n",
    "Do not change the provided prompt template.\n",
    "\n",
    "You are only allowed to change it in case of a possible extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSiH0Xqj79wc"
   },
   "source": [
    "### Optimizations\n",
    "\n",
    "Any kind of code optimization (e.g., speedup model inference or reduce computational cost) is more than welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cplnq3dLJne"
   },
   "source": [
    "### Bonus Points\n",
    "\n",
    "0.5 bonus points are arbitrarily assigned based on significant contributions such as:\n",
    "\n",
    "- Outstanding error analysis\n",
    "- Masterclass code organization\n",
    "- Evaluate A1 dataset and perform comparison\n",
    "- Perform prompt tuning\n",
    "\n",
    "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpr-LSK7LJnh"
   },
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
